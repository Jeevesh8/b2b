{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Drive-Less-b2b2b.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHFyytFFrcja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alfqKIjo4Mu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers pytorch-transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzWPTAyB3Ld6",
        "colab_type": "code",
        "outputId": "193641cf-413e-4517-ed8c-cc7b34c2d226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/b2b_\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/b2b_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba8ucXsOGNDi",
        "colab_type": "code",
        "outputId": "10a2e130-ae37-451b-f452-c4ac0567adb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone -b This-Branch 'https://Jeevesh8:Jeeveshy2000@github.com/Jeevesh8/b2b.git'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'b2b'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 244 (delta 69), reused 101 (delta 57), pack-reused 120\u001b[K\n",
            "Receiving objects: 100% (244/244), 222.80 KiB | 7.43 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSBrgNrfHAlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd b2b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIgKbLvAwJUv",
        "colab_type": "code",
        "outputId": "8ac798ac-d8c5-4083-ec16-b7d142d66afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!git fetch --all\n",
        "!git reset --hard origin/master\n",
        "!git pull origin master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching origin\n",
            "HEAD is now at 600efb1 Final Linear Layer for PLL_DAT added\n",
            "From https://github.com/Jeevesh8/b2b\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Switched to a new branch 'branch'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWnzpteHICNL",
        "colab_type": "code",
        "outputId": "ab370698-4c1a-47e2-b3a5-6d611d141a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!git checkout This-Branch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Switched to branch 'This-Branch'\n",
            "Your branch is behind 'origin/This-Branch' by 55 commits, and can be fast-forwarded.\n",
            "  (use \"git pull\" to update your local branch)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoCcEWwFIVB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v01d8llUJSYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.de\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/wmt14.en-de/train.en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpI_O3T4F_Lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from dataset import pll_datst, coll, mono_datst\n",
        "from preprocessing import load_data, tokenizer\n",
        "from model import xlmb2b\n",
        "from tqdm import tqdm\n",
        "from os import path\n",
        "from functools import partial\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import multiprocessing as mp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDv9FbnA3_lv",
        "colab_type": "code",
        "outputId": "f0175338-af42-4a85-95e0-efafebdc0578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "if path.exists(\"./file_1.csv\"):\n",
        "\tdata_obj = load_data(load_ = False)\n",
        "else:\n",
        "\tdata_obj = load_data(paths=['./train.en','./train.de'])\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "df_prllel, df_en, df_de = data_obj.final_data()\n",
        "pll_train_ds = pll_datst(df_prllel)\n",
        "mono_train_ds_en = mono_datst(df_en)\n",
        "mono_train_ds_de = mono_datst(df_de, lang='de')\n",
        "vocab_size = tokenizer.vocab_size"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-58806cfae1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_prllel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_de\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpll_train_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpll_datst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_prllel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmono_train_ds_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmono_datst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/b2b/preprocessing.py\u001b[0m in \u001b[0;36mfinal_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m             '''d = 0\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf_prllel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_eng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_de\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../data/file_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/file_0.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8k1Ci3c4HFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_sz = 4\n",
        "batch_size = 1\n",
        "d_model = 1024\n",
        "\n",
        "model_ed = xlmb2b(trfrmr_nlayers=1).double().to(device)\n",
        "model_de = xlmb2b(trfrmr_nlayers=1).double().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3-vJptR7KYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del model_ed.xlm\n",
        "model_ed.xlm = model_de.xlm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ3sWCrd4KJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cpus = 1 #mp.cpu_count()\n",
        "pll_train_loader = DataLoader(pll_train_ds,batch_size=b_sz, collate_fn = partial(coll, pll_dat = True), pin_memory=True, num_workers=cpus)\n",
        "mono_train_loader_en = DataLoader(mono_train_ds_en, batch_size=b_sz, collate_fn = partial(coll, pll_dat =False), pin_memory=True, num_workers=cpus)\n",
        "mono_train_loader_de = DataLoader(mono_train_ds_de, batch_size=b_sz, collate_fn = partial(coll, pll_dat =False), pin_memory=True, num_workers=cpus)\n",
        "optimizer_ed = torch.optim.Adam(model_ed.parameters(), lr = 0.01)\n",
        "optimizer_de = torch.optim.Adam(model_ed.parameters(), lr = 0.01)\n",
        "mseloss = nn.MSELoss()\n",
        "cross_entropy_loss = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLSzumT74gVy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_bleu(ref, cand, weights = (0.25, 0.25, 0.25, 0.25)):\n",
        "  \"\"\"\n",
        "     ref: (batch_size, seq_len, 1)\n",
        "     cand: (batch_size, seq_len, 1)\n",
        "  \"\"\"\n",
        "  references = []\n",
        "  candidates = []\n",
        "  dict_ = tokenizer.decoder\n",
        "  for i in range(ref.shape[0]):\n",
        "    refs = []\n",
        "    cands = []\n",
        "    for j in range(ref[i].shape[0]):\n",
        "      refs.append(dict_[ref[i][j]])\n",
        "      cands.append(dict_[cand[i][j]])\n",
        "    references.append([refs])\n",
        "    candidates.append(cands)\n",
        "\n",
        "  return corpus_bleu(references, candidates, weights)\n",
        "\n",
        "\n",
        "def reshape_n_edit(probs,out_mask) :\n",
        "  '''returns probs while removing rows with all 0 probs\n",
        "     the rows with all nan probs are due to padding of all\n",
        "     sequences to same length'''\n",
        "  out_mask = out_mask.repeat(1,1,vocab_size)\n",
        "  return probs[out_mask!=0].reshape(-1,vocab_size)\n",
        "\n",
        "def swap(batch,sr_embd,tr_embd,pll=True) :\n",
        "    if pll:\n",
        "        z2=batch['X']\n",
        "        z = batch['X']['input_ids'].clone()\n",
        "        z1 = batch['Y']['input_ids'].clone()\n",
        "        batch['X'] = batch['Y']\n",
        "        batch['Y'] = z2\n",
        "        batch['X']['input_ids'] = tr_embd\n",
        "        batch['Y']['input_ids'] = sr_embd\n",
        "        return batch, z, z1 \n",
        "\n",
        "    else:\n",
        "        z = batch['X']\n",
        "        batch['X']['input_ids'] = tr_embd\n",
        "\n",
        "    return batch, z['input_ids'], z\n",
        "\n",
        "def freeze_weights(model) :\n",
        "    for param in model.parameters() :\n",
        "        param.requires_grad = False\n",
        "\n",
        "def unfreeze_weights(model) :\n",
        "    for param in model.parameters() :\n",
        "        param.requires_grad = True\n",
        "\n",
        "def remove_pad_tokens(tensorr):\n",
        "    j = tokenizer.pad_token_id    \n",
        "    return tensorr[tensorr!=j]\n",
        "\n",
        "\n",
        "def set_to_eval(model_lis, beam_size=3) :\n",
        "    for model in model_lis :\n",
        "        model.eval()\n",
        "        model.beam_size = beam_size\n",
        "\n",
        "def send_to_gpu(batch, pll) :\n",
        "    lis =['X', 'Y'] if pll else ['X']\n",
        "    for elem in lis :\n",
        "        for key, value in batch[elem].items() :\n",
        "            batch[elem][key] = value.to(device, non_blocking=True)\n",
        "    return batch\n",
        "\n",
        "def evaluate(model, i, beam_size=3) :\n",
        "    set_to_eval(model,beam_size)\n",
        "\n",
        "def run(model_forward,model_backward,batch,optimizers,pll=True,send_trfrmr_out=False):\n",
        "    probs, sr_embd, tr_embd, trfrmr_out = model_forward(batch)\n",
        "    y = reshape_n_edit(probs)\n",
        "    m = remove_pad_tokens(batch['Y']['input_ids'].reshape(-1))\n",
        "    if pll : loss_pll = cross_entropy_loss(y,m)\n",
        "    del probs\n",
        "    if send_trfrmr_out :\n",
        "        batch, a, b = swap(batch, batch['X']['input_ids'], trfrmr_out, pll)\n",
        "    else :\n",
        "        batch, a, b  = swap(batch, sr_embd, tr_embd, pll)\n",
        "    probs_, sr_embd_, tr_embd_, trfrmr_out_ = model_backward(batch, not send_trfrmr_out)\n",
        "    y=reshape_n_edit(probs_, batch['Y']['attention_mask'])\n",
        "    m=remove_pad_tokens(a.reshape(-1))\n",
        "    loss_b2b = cross_entropy_loss(y, m) #since token_id is same as position in vocabulary\n",
        "    del probs_, sr_embd_, tr_embd_, trfrmr_out_,sr_embd,tr_embd,trfrmr_out\n",
        "    if pll : loss = loss_pll + loss_b2b\n",
        "    else : loss = loss_b2b\n",
        "    if torch.cuda.is_available() :\n",
        "        torch.cuda.synchronize()\n",
        "    for optimizer in optimizers :\n",
        "        optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for optimizer in optimizers :\n",
        "        optimizer.step()\n",
        "    return a,b,loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrqPVzVj3VQy",
        "colab_type": "code",
        "outputId": "8fdbf933-daee-4b1d-d815-6d886e305bbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "thresh_for_mono_data = 0.5\n",
        "losses_epochs = {\"pll\" : [], \"mono\": []}\n",
        "optimizers = [optimizer_de,optimizer_ed]\n",
        "thresh_for_xlm_weight_freeze = 0.7\n",
        "thresh_for_send_trfrmr_out = 0.9\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)) :\n",
        "\n",
        "    print(epoch)\n",
        "    model_ed.pll_dat=True\n",
        "    model_de.pll_dat=True\n",
        "    losses = [[], []]\n",
        "\n",
        "    for i, batch in enumerate(tqdm(pll_train_loader)) :\n",
        "        \n",
        "        batch = send_to_gpu(batch, pll=True)\n",
        "        #print(type(batch['X']))\n",
        "        batch['Y']['input_ids'], batch['X']['input_ids'], loss1 = run(model_ed,model_de,batch,optimizers)\n",
        "        losses[0].append(loss1.item())\n",
        "        del loss1\n",
        "        if torch.cuda.is_available() :\n",
        "            torch.cuda.synchronize()\n",
        "        #if epoch%20==0 : evaluate([model_ed,model_de], 1, beam_size=3)\n",
        "        batch['X']['attention_mask'] = (~(batch['X']['attention_mask'].bool())).float()\n",
        "        batch['Y']['attention_mask'] = (~(batch['Y']['attention_mask'].bool())).float()\n",
        "        _,_,loss2 = run(model_de,model_ed,batch,optimizers)\n",
        "        #if epoch%20==0 : evaluate([model_ed,model_de], 2, beam_size=3)\n",
        "        losses[1].append(loss2.item())\n",
        "        del loss2\n",
        "        if torch.cuda.is_available() :\n",
        "            torch.cuda.synchronize()\n",
        "        if i%50==0 and i!=0 :\n",
        "            print(sum(losses[0][-100:-1])/100, sum(losses[1][-100:-1])/100)\n",
        "\n",
        "    losses_epochs['pll'].append([losses[0].sum()/len(losses[0]), losses[1].sum()/len(losses[1])])\n",
        "\n",
        "#Training on monolingual data if the above losses are sufficiently low:\n",
        "\n",
        "    if(losses_epochs['pll'][-1][0]<thresh_for_mono_data or losses['pll'][-1][1]<thresh_for_mono_data):\n",
        "\n",
        "        print(\"Going for Monolingual Training\")\n",
        "\n",
        "        model_ed.pll_dat = False\n",
        "        model_de.pll_dat = False\n",
        "        losses = [[], []]\n",
        "\n",
        "        for i, batch in enumerate(mono_train_loader_en):\n",
        "            batch = send_to_gpu(batch, pll=False)\n",
        "\n",
        "            _,_,loss1 = run(model_ed,model_de,batch,optimizers,pll=False)\n",
        "\n",
        "            losses[0].append(loss1)\n",
        "\n",
        "        for i, batch in enumerate(mono_train_loader_de):\n",
        "            batch = send_to_gpu(batch, pll=False)\n",
        "\n",
        "            _,_,loss2 = run(model_de,model_ed,batch,optimizers,pll=False)\n",
        "\n",
        "            losses[1].append(loss2)\n",
        "\n",
        "        losses_epochs['mono'].append([losses[0].sum()/len(losses[0]), losses[1].sum()/len(losses[1])])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
            "  0%|          | 0/25000 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[A\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-0cb20801b7ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_to_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#print(type(batch['X']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_de\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ac82967abc6b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model_forward, model_backward, batch, optimizers, pll, send_trfrmr_out)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape_n_edit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_pad_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mpll\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mloss_pll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_trfrmr_out\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 916\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 1836\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   1837\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (232) to match target batch_size (121)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WSMQ7ScC9co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = nn.Softmax(dim=1)\n",
        "m = y(torch.tensor([[1.,1],[2,2]]))\n",
        "m[m==0.5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdiYkleeUieX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.cat([torch.ones(3),torch.zeros(0)],dim=0)\n",
        "x[x==0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MVZ4C6rhxVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "0*-np.inf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI3VQ2Dmo4Di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "batch_2_in = torch.zeros(2, 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bN3xYxTvNwl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_2_in.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDd9O_zSu1td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "from transformers import XLMTokenizer, XLMWithLMHeadModel, XLMModel\n",
        "xlm = XLMModel.from_pretrained('xlm-mlm-ende-1024').to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBcHuDy79AGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xlm2 =  XLMModel.from_pretrained('xlm-mlm-ende-1024').to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw6l8BSM9EVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del xlm\n",
        "xlm = xlm2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7eHYoGb4bCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dic = {'input_ids':batch_2_in.long(), \n",
        "       'langs' : torch.LongTensor([[0]*20]*2), \n",
        "       'position_ids' : torch.LongTensor([[i for i in range(20)]]*2), \n",
        "       'attention_mask':torch.FloatTensor([[1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0]]*2), \n",
        "       'lengths' : torch.LongTensor([10,10]) }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1KiVxyb5-kE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k,v in dic.items() :\n",
        "    dic[k] = v.pin_memory()\n",
        "for k,v in dic.items() :\n",
        "    dic[k]=v.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVo7N93zvABY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xlm(**dic)[0].shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ5Wd1Pc4R0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = 3\n",
        "x = torch.LongTensor([ [[3]*10] for i in range(20)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6V6PTvEH-HJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = torch.tensor([1,2])\n",
        "print(int(x.max()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy05gOyjJydI",
        "colab_type": "code",
        "outputId": "a194f420-afa2-416d-829f-f13bcc22aae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import torch\n",
        "probs= torch.tensor([[[1,2,3],[4,5,6]],[[3,4,5],[2,2,2]]])\n",
        "mask = torch.tensor([[1,1],[1,0]])\n",
        "probs[mask!=0].reshape(-1, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6],\n",
              "        [3, 4, 5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWyi9moBOqNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyaED5E1P0oO",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}